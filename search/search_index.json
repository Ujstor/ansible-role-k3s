{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K3s Ansible Playbooks","text":"<p>Ansible role for managing rancher k3s, lightweight, cncf-certified kubernetes distribution. This role can be used to install simple single-node or multi-master HA clusters. It can be used to manage multiple k3s clusters in single ansible inventory. It's also heavily customizable for almost any purpose - you can edit pretty much any k3s setting. It can install gvisor, additional host dependencies, load specific kernel modules, adjust k3s-related sysctl settings and so on.</p> <p>Apart from what k3s requires, this role also needs systemd, so it should work on any modern distro.  </p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Run Ansible with Docker</li> <li>SSH Keys</li> <li>Run Playbooks</li> <li>Variables</li> </ol>"},{"location":"#1-run-ansible-with-docker","title":"1. Run Ansible with Docker","text":"<p>Back to Top</p> <p>A Docker container is the best way to manage the Ansible configuration, dependencies  and to execute playbooks.</p> <p>Be sure to mount the SSH key that is responsible for host management.</p> <pre><code>docker build -t k3s-ansible .\n\ndocker run --rm -it \\\n  --mount type=bind,source=\"${HOME}\"/.ssh/k3s_ansible_rsa.pem,destination=/root/.ssh/k3s_ansible_rsa.pem \\\n  k3s-ansible bash\n\nsource env/bin/activate\n</code></pre> <p>You are ready for executing playooks.</p> <p>If the configuration is changed, rebuilding the Docker image.</p>"},{"location":"#2-ssh-keys","title":"2. SSH keys","text":"<p>Back to Top</p> <p>SSH keys associated with nodes, when downloaded from RDM, are in OpenSSH format. They need to be converted into RSA format.</p> <p>If the servers do not share the same SSH key, each key needs to be specified in the hosts file as the ansible_ssh_private_key_file property. Otherwise, it should be defined in the .cfg config with private_key_file. Also, if you SSH into servers as a non-root user, you will need to define the users for SSH in the host inventory:</p> <pre><code>[k3s_master]\n138.201.118.66 ansible_ssh_private_key_file: ~/.ssh/k3s_ansible_rsa ansible_user: administrator\n</code></pre> <p>A better approach is to have the same SSH key for the entire cluster.</p> <pre><code>private_key_file = ~/.ssh/k3s_ansible_rsa\n</code></pre>"},{"location":"#3-run-playbooks","title":"3. Run Playbooks","text":"<p>Back to Top</p> <p>Host configuration is pulled from <code>hosts.ini</code>.</p> <pre><code>ansible-playbook k3s_playbook.yml\n</code></pre>"},{"location":"#4-variables","title":"4. Variables","text":"<p>Back to Top</p> Variable name Default value Description k3s_version <code>v1.30.3+k3s1</code> version of k3s to install k3s_systemd_dir /etc/systemd/system Directory for systemd unit file k3s_master <code>false</code> installs k3s master when true k3s_agent <code>false</code> installs k3s agent when true k3s_master_ip first node in k3s_master_group group ip/hostname of master node k3s_master_port <code>6443</code> port of masterserver k3s_install_mode <code>online</code> k3s install mode - online or airgap k3s_flannel_backend <code>vxlan</code> k3s flannel backend to use. Set to none to disable flannel k3s_master_disable <code>[]</code> array of k3s packaged components to disable (traefik,metrics-server,etc) k3s_master_extra_args <code>[]</code> extra arguments for k3s server (official docs) k3s_master_extra_config `` YAML with extra config for k3s master k3s_agent_extra_config `` YAML with extra config for k3s agent k3s_kubelet_extra_config `` Additional arguments for kubelet, see docs k3s_agent_extra_args <code>[]</code> extra arguments for k3s agent (official docs) k3s_extra_config_files <code>{}</code> extra configfiles for k3s k3s_bpffs <code>false</code> mounts /sys/fs/bpf bpffs (needed by some network stacks) k3s_external_ip `` specifies k3s external ip k3s_internal_ip `` specifies k3s node ip k3s_registries `` Configures custom registries, see official docs for format k3s_cronjob_prune_images <code>absent</code> Configures cronjob that prunes unused images in containerd daily. Either <code>absent</code> or <code>present</code> k3s_gvisor <code>false</code> Installs gvisor k3s_gvisor_version <code>20240807</code> gvisor version to install k3s_gvisor_platform <code>systrap</code> Selects platform to use in gvisor k3s_gvisor_config `` Sets additional options for gvisor runsc. See notes k3s_gvisor_create_runtimeclass <code>true</code> Automatically create gvisor RuntimeClass in kubernetes k3s_kubeconfig false Downloads kubeconfig to machine from which role was launched k3s_kubeconfig_server see below specifies server for use in kubeconfig k3s_kubeconfig_context k3s specifies context to use in kubeconfig k3s_kubeconfig_target: <code>{{ k3s_kubeconfig_context }}</code> specifies filename for downloading kubeconfig k3s_agent_group k3s_node specifies ansible group name for k3s nodes k3s_master_group k3s_master specifies ansible group name for k3s master(s) k3s_extra_packages <code>[]</code> Installs additional packages if needed by workloads (ie iscsid) k3s_extra_services <code>[]</code> Enables additional services if needed by workloads (ie iscsid) k3s_extra_config_files <code>{}</code> additional config files for kubelet/kubeapi k3s_sysctl_config <code>{}</code> Allows setting arbitrary sysctl settings k3s_extra_manifests <code>{}</code> Allows applying kubernetes manifests"},{"location":"advanced-configuration/additional-k8s-configs/","title":"Creating additional kubernetes configs","text":"<p>Sometimes you need to create additional config files for adding to kubernetes installation. For example, you want to trace kubelet, which requires separate config file for tracing configuration. Variable <code>k3s_extra_config_files</code> will take care of that. All additional config files will go to <code>/etc/rancher/k3s</code> directory, with name specified in name block and with content specified in content. This action will happen on pre-configuration stage, before k3s installation.  </p> <p>Example:</p> <pre><code>k3s_extra_config_files:\n  - name: apiserver-tracing.yaml\n    content: |\n      apiVersion: apiserver.config.k8s.io/v1alpha1\n      kind: TracingConfiguration\n      endpoint: 127.0.0.1:4317\n      samplingRatePerMillion: 100\n</code></pre> <p>Will result in file <code>/etc/rancher/k3s/apiserver-tracing.yaml</code> with following content:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1alpha1\nkind: TracingConfiguration\nendpoint: 127.0.0.1:4317\nsamplingRatePerMillion: 100\n</code></pre> <p>Please note that no additional formatting or processing is happening on that stage, so you need to take care about all indentation and other formatting things. Additionally, editing any of those files will trigger k3s restart</p>"},{"location":"advanced-configuration/additional-packages-and-services/","title":"Additional packages and services","text":"<p>Sometimes certain software requires certain packages installed on host system. Some of examples are distributed filesystems like longhorn and openebs which require iscsid. While it's better to manage such software with dedicated roles, i included that variables for simplicity. If you want openebs-jiva or longhorn to work, you can specify</p> <pre><code>k3s_extra_packages:\n  - open-iscsi\nk3s_extra_services:\n  - iscsid\n</code></pre> <p>open-iscsi will be installed and iscsid service will be both started and enabled at boot-time before k3s installation</p>"},{"location":"advanced-configuration/containerd-template/","title":"Customizing containerd config template","text":"<p>If you use different version of k3s and/or you want to customize containerd template, you can override path to containerd template with <code>k3s_containerd_template</code> variable, for example: </p> <pre><code>k3s_containerd_template: \"{{ inventory_dir }}/files/k3s/containerd.toml.tmpl.j2\"\n</code></pre> <p>In that case, role will look for containerd template in <code>files/k3s/containerd.toml.tmpl.j2</code> inside your inventory folder you defined in <code>ansible.cfg</code> </p>"},{"location":"advanced-configuration/custom-cni/","title":"Using custom network plugin","text":"<p>If you want to use something different and self-managed than default flannel you can set flannel backend to none, which will remove flannel completely:</p> <pre><code>k3s_flannel_backend: none\n</code></pre> <p>Additionally, if you want to use something with eBPF dataplane enabled (calico, cilium) you might need to disable kube-proxy and mount bpffs filesystem on host node:</p> <pre><code>k3s_bpffs: true\nk3s_master_extra_config:\n  disable-kube-proxy: true\n</code></pre>"},{"location":"advanced-configuration/custom-manifests/","title":"Adding custom kubernetes manifests","text":"<p>If you need to create additional kubernetes objects after cluster creation, you can use k3s_extra_manifests variable.  Example with all possible parameters:</p> <pre><code>k3s_extra_manifests:\n  - name: kata\n    state: present\n    definition:\n      apiVersion: node.k8s.io/v1\n      kind: RuntimeClass\n      metadata:\n        name: kata\n      handler: kata\n</code></pre> <p>You can supply full definition in \"definition\" block, including resource name in metadata.name (kata in example). If your object doesn't contain metadata.name, then name from ansible will be used (kata in example). Object name in .definition have precedence and will be used if both .name and .definition.metadata.name exists. You can also control control resource state with state parameter (<code>absent</code>, <code>present</code>), which is set to <code>present</code> by default.  Object creation will be delegated to first node in your <code>k3s_master_group</code>, in case of multi-master setup it will be your \"initial\" master node. For RBAC, it will use k3s-generated <code>/etc/rancher/k3s/k3s.yaml</code> kubeconfig on same master server, which have cluster-admin rights.</p>"},{"location":"advanced-configuration/custom-registries/","title":"Adding custom registries","text":"<p>By using k3s_registries variable you can configure custom registries, both origins and mirrors. Format follows official config format. Example:</p> <pre><code>k3s_registries:\n  mirrors:\n    docker.io:\n      endpoint:\n        - \"https://mycustomreg.com:5000\"\n  configs:\n    \"mycustomreg:5000\":\n      auth:\n        username: xxxxxx # this is the registry username\n        password: xxxxxx # this is the registry password\n      tls:\n        cert_file: # path to the cert file used in the registry\n        key_file:  # path to the key file used in the registry\n        ca_file:   # path to the ca file used in the registry\n</code></pre>"},{"location":"advanced-configuration/external-cloud-controller/","title":"Provisioning cluster using external cloud-controller-manager","text":"<p>By default, cluster will be installed with k3s \"dummy\" cloud controller manager. If you deploy your k3s cluster on supported cloud platform (for example hetzner with their ccm) you will need to specify following parameters before first cluster start, since cloud controller can't be changed after cluster deployment:</p> <pre><code>k3s_master_extra_config:\n  disable-cloud-controller: true\nk3s_kubelet_extra_config:\n  - \"cloud-provider=external\"\n</code></pre>"},{"location":"advanced-configuration/gvisor/","title":"Gvisor Setup","text":"<p>By setting k3s_gvisor to true role will install gvisor - google's application kernel for container.  By default it will use systrap mode, to switch it to kvm set k3s_gvisor_platform to kvm. If platform will be set to kvm, role will also load (and persist) corresponding module into kernel. It will also create RuntimeClass kubernetes object if you have variable k3s_gvisor_create_runtimeclass set to true (default). If you want to create it manually:</p> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\n</code></pre> <p>After that you should be able to launch gvisor-enabled pods by adding runtimeClassName to pod spec, ie</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gvisor-nginx\nspec:\n  runtimeClassName: gvisor\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre>"},{"location":"advanced-configuration/gvisor/#additional-configuration","title":"Additional configuration","text":"<p>Role supports passing additional settings for gvisor using <code>k3s_gvisor_config</code>. For example, to enable host networking, use:</p> <pre><code>k3s_gvisor_config:\n  network: host\n</code></pre> <p>Which will become</p> <pre><code>[runsc_config]\n  network = \"host\"\n</code></pre> <p>in gvisor config</p>"},{"location":"advanced-configuration/setting-kubelet-arguments/","title":"Setting kubelet arguments","text":"<p>To pass arguments for kubelet, you can use <code>k3s_kubelet_extra_config</code> variable:</p> <pre><code>k3s_kubelet_extra_config:\n  - \"image-gc-high-threshold=40\"\n  - \"image-gc-low-threshold=30\"\n</code></pre>"},{"location":"advanced-configuration/setting-sysctl/","title":"Sysctl Settings","text":""},{"location":"advanced-configuration/setting-sysctl/#setting-sysctl","title":"Setting sysctl","text":"<p>Role also allows setting arbitrary sysctl settings using <code>k3s_sysctl_config</code> variable in dict format:</p> <pre><code>k3s_sysctl_configs:\n  fs.inotify.max_user_instances: 128\n</code></pre> <p>Settings defined with that varible will be persisted in <code>/etc/sysctl.d/99-k3s.conf</code> file, loading them after system reboots</p>"},{"location":"advanced-configuration/specifying-ip/","title":"k3s and external ip","text":"<p>Sometimes k3s fails to properly detect external and internal ip. For those, you can use <code>k3s_external_ip</code> and <code>k3s_internal_ip</code> variables, for example: Ie:</p> <pre><code>k3s_external_ip: \"{{ ansible_default_ipv4['address'] }}\"\nk3s_internal_ip: \"{{ ansible_vpn0.ipv4.address }}\"\n</code></pre> <p>In which case external ip will be ansible default ip and node ip (internal-ip) will be ip address of vpn0 interface</p>"},{"location":"install/airgapped_install/","title":"Airgapped Install","text":"<p>For environments without internet access, you can use</p> <pre><code>k3s_install_mode: airgap\n</code></pre> <p>In that mode, role will download k3s binary and bootstrap images locally, and transfer them to target server from ansible runner. It will also work for gvisor. Please note that if you use additional manifests installation, you will need python3-kubernetes package installed on system - role assumes you have accessible OS distribution mirror configured on that airgapped node, otherwise installation will fail. If you can't get that package installed on your system, do not use automatic installation of manifests and set</p> <pre><code>k3s_gvisor_create_runtimeclass: false\n</code></pre>"},{"location":"install/basics/","title":"Basic","text":"<p>This role discovers installation mode from your ansible inventory.  For working with your inventory, it operates on two basic variables,  <code>k3s_master_group</code> and <code>k3s_agent_group</code>, which are set to <code>k3s_master</code> and <code>k3s_agent</code> by default.</p> <p>Following is an example of single master and 2 agents:</p> <pre><code>[k3s_master]\nkube-master-1.example.org\n\n[k3s_agent]\nkube-node-1.example.org\nkube-node-2.example.org\n</code></pre> <p>For group with master, k3s_master in that example, you should enable master installation with <code>k3s_master</code> variable:</p> <pre><code>k3s_master: true\n</code></pre> <p>Accordingly, for agents, use <code>k3s_agent</code> variable:</p> <pre><code>k3s_agent: true\n</code></pre> <p>For selecting master server to connect, you can use <code>k3s_master_ip</code> variable. By default it will be set to first ansible_host in ansible group specified in <code>k3s_master_group</code> variable. Of course, you can always redefine it manually.</p> <pre><code>- name: k3s multi node - master\n  hosts: k3s_master\n  become: yes\n  become_user: root\n  gather_facts: False\n  roles:\n    - role: k3s\n  vars:\n    k3s_master: true\n    k3s_kubeconfig: true\n    k3s_master_extra_args:\n      - \"--token alskdfjasljkfdlasjflakjsdflkj\"\n\n- name: k3s multi node - agent\n  hosts: k3s_agent\n  become: yes\n  become_user: root\n  gather_facts: False\n  roles:\n    - role: k3s\n  vars:\n    k3s_agent: true\n    k3s_agent_extra_args:\n      - \"--server https://kube-master-1.example.org:6443\"\n      - \"--token alskdfjasljkfdlasjflakjsdflkj\"\n</code></pre>"},{"location":"install/getting-kubeconfig/","title":"Getting kubeconfig via role","text":"<p>Role have ability to download kubeconfig file to machine from where ansible was run. To use it, set following variables:</p> <pre><code>k3s_kubeconfig: true\nk3s_kubeconfig_context: k3s-de1\n</code></pre> <p>Role will perform following:</p> <ol> <li> <p>Copy <code>/etc/rancher/k3s/k3s.yml</code> to <code>~/.kube/config-${ k3s_kubeconfig_context</code> }</p> </li> <li> <p>Patch it with your preferred context name specified in <code>k3s_kubeconfig_context</code> variable instead of stock <code>default</code></p> </li> <li> <p>Patch it with proper server URL (by default, it will be ansible_host of first master node in group specified in variable <code>k3s_master_group</code>, with port 6443, aka \"initial master\"), but you can override it with <code>k3s_kubeconfig_server</code></p> </li> <li> <p>Download resulting file to machine running ansible with path <code>~/.kube/config-${ k3s_kubeconfig_context }</code>, in current example it will be <code>~/.kube/config-k3s-de1</code></p> </li> </ol> <p>And you can start using it right away. However, if your master is configured differently (HA IP, Load balancer, etc), you might want to specify server manually. For this, you can use <code>k3s_kubeconfig_server</code> variable:</p> <pre><code>k3s_kubeconfig_server: \"master-ha.k8s.example.org:6443\"\n</code></pre> <p>Please note that role will not track changes of <code>/etc/rancher/k3s/k3s.yml</code> - if you redeploy your k3s cluster and need new kubeconfig, just delete existing local kubeconfig to get new one.</p>"},{"location":"install/multi_master/","title":"Multi Master install","text":"<p>When your <code>k3s_master</code> ansible inventory group have more than 1 host, role will detect it and switch to multi-master installation. First node in group will be used as \"bootstrap\", while following will bootstrap from first node. Any number of nodes is ok, but it's generally recommended to have odd number of nodes for etcd eletction to work, since etcd quorum is (n/2)+1 where n is number of nodes.  </p> <p>You can also switch existing, single-node sqlite master to multimaster configuration by adding more masters to existing install - be aware, however, that migration from single-node sqlite to etcd is supported only in k3s &gt;= 1.22!</p> <p>Pay attention that in default configuration all agents will be pointing only to first master, which is not really useful for HA setup. Configuring HA is out of scope for this role, so take a look at following docs:</p>"},{"location":"install/multi_master/#ha-with-haproxy","title":"HA with haproxy","text":"<p>Using This haproxy role. I run my cluster on top of L3 vpn so i can't use L2, so i just install haproxy on each node, point haproxy to all masters, and point agents to localhost haproxy. Dirty, but works. Example config:</p> <pre><code>haproxy_listen:\n  - name: stats\n    description: Global statistics\n    bind:\n      - listen: '0.0.0.0:1936'\n    mode: http\n    http_request:\n      - action: use-service\n        param: prometheus-exporter\n        cond: if { path /metrics }\n    stats:\n      enable: true\n      uri: /\n      options:\n        - hide-version\n        - show-node\n      admin: if LOCALHOST\n      refresh: 5s\n      auth:\n        - user: admin\n          passwd: 'yoursupersecretpassword'\nhaproxy_frontend:\n  - name: kubernetes_master_kube_api\n    description: frontend with k8s api masters\n    bind:\n      - listen: \"127.0.0.1:16443\"\n    mode: tcp\n    default_backend: k8s-de1-kube-api\nhaproxy_backend:\n  - name: k8s-de1-kube-api\n    description: backend with all kubernetes masters\n    mode: tcp\n    balance: roundrobin\n    option:\n      - httpchk GET /readyz\n    http_check: expect status 401\n    default_server_params:\n      - inter 1000\n      - rise 2\n      - fall 2\n    server:\n      - name: k8s-de1-master-1\n        listen: \"master-1:6443\"\n        param:\n          - check\n          - check-ssl\n          - verify none\n      - name: k8s-de1-master-2\n        listen: \"master-2:6443\"\n        param:\n          - check\n          - check-ssl\n          - verify none\n      - name: k8s-de1-master-3\n        listen: \"master-3:6443\"\n        param:\n          - check\n          - check-ssl\n          - verify none\n</code></pre> <p>That will start haproxy listening on 127.0.0.1:16443 for connections to k8s masters. You can then redefine master IP and port for agents with</p> <pre><code>k3s_master_ip: 127.0.0.1\nk3s_master_port: 16443\n</code></pre> <p>And now your connections are balanced between masters and protected in case of one or two masters will go down. One downside of that config is that it checks for reply 401 on /readyz endpoint, because since certain version of k8s (1.19 if i recall correctly) this endpoint requires authorization. So you have 2 options here:</p> <ul> <li>Continue to rely on 401 check (not a good solution, since we're just checking for http up status)</li> <li>Add <code>anonymous-auth=true</code> to apiserver arguments:        <code>yaml         k3s_master_extra_config:           kube-apiserver-arg:           - \"anonymous-auth=true\"</code>     This will open /readyz, /healthz, /livez and /version endpoints to anonymous auth, and potentially expose version info. If that is concerning you, it's possible to patch system:public-info-viewer role to keep only /readyz, /healthz and /livez endpoint open:     <code>kubectl patch clusterrole system:public-info-viewer --type=json -p='[{\"op\": \"replace\", \"path\": \"/rules/0/nonResourceURLs\", \"value\":[\"/healthz\",\"/livez\",\"/readyz\"]}]'</code></li> </ul> <p>This proxy also works with initial agent join, so it's better to setup haproxy before installing k3s and then switching to HA config. It will also expose prometheus metrics on 0.0.0.0:1936/metrics - pay attention that this part (unlike webui) won't be protected by user and password, so adjust your firewall accordingly if needed!</p> <p>Of course you can use whatever you want - external cloud LB, nginx, anything, all it needs is TCP protocol support (because in this case we don't want to manage SSL on loadbalancer side). But haproxy provides you with prometheus metrics, have nice webui for monitoring and management, and i'm just familiar with it.</p>"},{"location":"install/multi_master/#ha-with-vrrp-keepalived","title":"HA with VRRP (keepalived)","text":"<p>You can use this keepalived role if you have L2 networking available and can use VRRP for failover IP. In that case, you might need to add tls-san option in k3s_master_extra_config with your floating ip. For keepalived to work, following conditions should be met:   1) L2 networking must be available. Sadly, this is not a common case with cloud providers and most VPNs.   2) Virtual IP must be in same subnet as interfaces on top of which they are used</p> <p>Sample keepalived configuration on master-1, assuming we use network 10.91.91.0/24 on vpn0 interface:</p> <pre><code>keepalived_instances:\n  vpn:\n    interface: vpn0\n    state: MASTER\n    virtual_router_id: 51 #if you have multiple VRRP setups in same network this should be unique\n    priority: 255 #node usually owning IP should always have priority set to 255\n    authentication_password: \"somepassword\" #can be omitted, but always good to use\n    vips:\n      - \"10.91.91.50 dev vpn0 label vpn0:0\"\n</code></pre> <p>for backing masters:</p> <pre><code>keepalived_instances:\n  vpn:\n    interface: vpn0\n    state: BACKUP\n    virtual_router_id: 51\n    priority: 254 #use lower priority for each node\n    authentication_password: \"somepassword\"\n    vips:\n      - \"10.91.91.50 dev vpn0 label vpn0:0\"\n</code></pre> <p>And in k3s configuration:</p> <pre><code>k3s_master_extra_config:\n  tls-san: 10.91.91.50\n</code></pre> <p>If everything is configured correctly, you should see 10.91.91.50 on vpn0:0 interface on master-1 node. Try stopping keepalived on master-1 and see how IP disappears from master-1 and appears on master-2. From now it's your choice how you want to configure HA - point agents to that floating IP, or install load-balancer on each master node and distribute requests between them.</p>"}]}